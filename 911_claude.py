# -*- coding: utf-8 -*-
"""911_Claude.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EqHLwYt84MsdryXZ5-QNQhFmHur_aChI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime
import re
import warnings
warnings.filterwarnings('ignore')
from sklearn.inspection import permutation_importance

# Set plotting style
plt.style.use('ggplot')
sns.set(style='whitegrid')

# Load the dataset
def load_data(file_path):
    df = pd.read_csv(file_path, delimiter='\t')
    print(f"Dataset loaded with shape: {df.shape}")
    return df

# Data preprocessing
def preprocess_data(df):
    # Make a copy to avoid modifying the original dataframe
    data = df.copy()

    # Drop unnecessary columns
    print("Original columns:", data.columns.tolist())

    # Convert callDateTime to datetime format
    data['callDateTime'] = pd.to_datetime(data['callDateTime'], errors='coerce')

    # Extract date and time features
    data['call_date'] = data['callDateTime'].dt.date
    data['call_hour'] = data['callDateTime'].dt.hour
    data['call_day'] = data['callDateTime'].dt.day_of_week
    data['call_month'] = data['callDateTime'].dt.month
    data['is_weekend'] = data['call_day'].apply(lambda x: 1 if x >= 5 else 0)  # 5,6 represent weekend (Sat, Sun)

    # Create the target variable (1 for fake call, 0 for real call)
    data['is_fake'] = data['priority'].apply(lambda x: 1 if x in ['Low', 'Non-Emergency'] else 0)

    # Print unique priority values to verify our categorization
    print("\nUnique priority values:", data['priority'].unique())
    print("\nTarget variable distribution:")
    print(data['is_fake'].value_counts())

    return data

# Exploratory Data Analysis
def explore_data(data):
    # Display basic information
    print("\n--- Basic Information ---")
    print(data.info())

    # Descriptive statistics
    print("\n--- Descriptive Statistics ---")
    print(data.describe())

    # Check for missing values
    print("\n--- Missing Values ---")
    missing_values = data.isnull().sum()
    print(missing_values[missing_values > 0])

    # Print first few rows
    print("\n--- Sample Data ---")
    print(data.head())

    return data

# Visualize data
def visualize_data(data):
    print("\n--- Data Visualization ---")

    # Set up the visualization area
    plt.figure(figsize=(20, 15))

    # Plot 1: Distribution of fake vs. real calls
    plt.subplot(2, 3, 1)
    sns.countplot(x='is_fake', data=data)
    plt.title('Distribution of Fake vs. Real Calls')
    plt.xlabel('Is Fake (1=Yes, 0=No)')
    plt.ylabel('Count')

    # Plot 2: Distribution of priorities
    plt.subplot(2, 3, 2)
    sns.countplot(y='priority', data=data, order=data['priority'].value_counts().index)
    plt.title('Distribution of Call Priorities')
    plt.xlabel('Count')
    plt.ylabel('Priority')

    # Plot 3: Call distribution by hour
    plt.subplot(2, 3, 3)
    sns.countplot(x='call_hour', data=data, hue='is_fake')
    plt.title('Call Distribution by Hour (Fake vs. Real)')
    plt.xlabel('Hour of Day')
    plt.ylabel('Count')
    plt.legend(title='Is Fake')

    # Plot 4: Call distribution by day of week
    plt.subplot(2, 3, 4)
    day_of_week = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}
    data['day_name'] = data['call_day'].map(day_of_week)
    sns.countplot(x='day_name', data=data, hue='is_fake',
                  order=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
    plt.title('Call Distribution by Day of Week (Fake vs. Real)')
    plt.xlabel('Day of Week')
    plt.ylabel('Count')
    plt.legend(title='Is Fake')

    # Plot 5: Top 10 descriptions for fake calls
    plt.subplot(2, 3, 5)
    fake_calls = data[data['is_fake'] == 1]
    top_fake_desc = fake_calls['description'].value_counts().head(10)
    sns.barplot(y=top_fake_desc.index, x=top_fake_desc.values)
    plt.title('Top 10 Descriptions for Fake Calls')
    plt.xlabel('Count')
    plt.ylabel('Description')

    # Plot 6: Top 10 descriptions for real calls
    plt.subplot(2, 3, 6)
    real_calls = data[data['is_fake'] == 0]
    top_real_desc = real_calls['description'].value_counts().head(10)
    sns.barplot(y=top_real_desc.index, x=top_real_desc.values)
    plt.title('Top 10 Descriptions for Real Calls')
    plt.xlabel('Count')
    plt.ylabel('Description')

    plt.tight_layout()
    plt.savefig('call_visualization.png')
    plt.show()

    # Additional visualizations
    plt.figure(figsize=(20, 10))

    # Plot 7: Distribution of fake vs. real calls by district
    plt.subplot(2, 2, 1)
    district_fake = pd.crosstab(data['district'], data['is_fake'])
    district_fake_pct = district_fake.div(district_fake.sum(axis=1), axis=0)
    district_fake_pct.plot(kind='bar', stacked=False, ax=plt.gca())
    plt.title('Percentage of Fake Calls by District')
    plt.xlabel('District')
    plt.ylabel('Percentage')
    plt.legend(title='Is Fake')

    # Plot 8: Distribution of fake vs. real calls by police district
    plt.subplot(2, 2, 2)
    pd_fake = pd.crosstab(data['PoliceDistrict'], data['is_fake'])
    pd_fake_pct = pd_fake.div(pd_fake.sum(axis=1), axis=0)
    pd_fake_pct.plot(kind='bar', stacked=False, ax=plt.gca())
    plt.title('Percentage of Fake Calls by Police District')
    plt.xlabel('Police District')
    plt.ylabel('Percentage')
    plt.legend(title='Is Fake')

    # Plot 9: Distribution of call types
    plt.subplot(2, 2, 3)
    top_descriptions = data['description'].value_counts().head(15)
    sns.barplot(y=top_descriptions.index, x=top_descriptions.values)
    plt.title('Top 15 Call Descriptions')
    plt.xlabel('Count')
    plt.ylabel('Description')

    # Plot 10: Month-wise distribution
    plt.subplot(2, 2, 4)
    sns.countplot(x='call_month', data=data, hue='is_fake')
    plt.title('Call Distribution by Month (Fake vs. Real)')
    plt.xlabel('Month')
    plt.ylabel('Count')
    plt.legend(title='Is Fake')

    plt.tight_layout()
    plt.savefig('additional_visualization.png')
    plt.show()

    return data

def engineer_features(data):
    print("\n--- Feature Engineering ---")

    # Create a copy to avoid modifying the original dataframe
    df_fe = data.copy()

    # Create categorical features from description using TF-IDF
    print("Creating TF-IDF features from description...")
    # Fill NaN values in description with an empty string
    tfidf = TfidfVectorizer(max_features=100, stop_words='english')
    description_features = tfidf.fit_transform(df_fe['description'].fillna('MISSING_DESCRIPTION'))

    global_tfidf_vectorizer = tfidf

    # Convert to DataFrame and concatenate
    description_df = pd.DataFrame(description_features.toarray(),
                                 columns=[f'desc_{i}' for i in range(description_features.shape[1])])

    # Handle missing values in district and police district before one-hot encoding
    df_fe['district'] = df_fe['district'].fillna('UNKNOWN_DISTRICT')
    df_fe['PoliceDistrict'] = df_fe['PoliceDistrict'].fillna('UNKNOWN_PD')

    # Get one-hot encoding of district
    district_dummies = pd.get_dummies(df_fe['district'], prefix='district')

    # Get one-hot encoding of police district
    pd_dummies = pd.get_dummies(df_fe['PoliceDistrict'], prefix='pd')

    # Fill missing values in numerical features
    numerical_features = df_fe[['call_hour', 'call_day', 'call_month', 'is_weekend']].fillna(-1)

    # Combine all features
    combined_features = pd.concat([
        numerical_features,
        district_dummies,
        pd_dummies,
        description_df
    ], axis=1)

    print(f"Feature engineering completed. Feature shape: {combined_features.shape}")

    # Check for any remaining NaN values
    nan_check = combined_features.isnull().sum().sum()
    if nan_check > 0:
        print(f"WARNING: {nan_check} NaN values still present after feature engineering")
        print("Columns with NaN values:")
        print(combined_features.columns[combined_features.isnull().any()].tolist())
        # Apply a final imputation to any remaining NaN values
        combined_features = combined_features.fillna(-999)
    else:
        print("No NaN values present in features - data is ready for modeling")

    return combined_features, df_fe['is_fake']

# Train-test split function
def split_data(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,
                                                        random_state=random_state,
                                                        stratify=y)
    print(f"Training set shape: {X_train.shape}")
    print(f"Testing set shape: {X_test.shape}")
    return X_train, X_test, y_train, y_test

# Build and train models with improved handling of missing values
def build_models(X_train, X_test, y_train, y_test):
    print("\n--- Building Models ---")

    # Dictionary to store models and their performances
    model_results = {}

    # Models to try
    models = {
        'Random Forest': RandomForestClassifier(random_state=42),
        'Gradient Boosting': HistGradientBoostingClassifier(random_state=42),  # Changed to HistGradientBoosting which handles NaN values
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=500)
    }

    # Train and evaluate each model
    for name, model in models.items():
        print(f"\nTraining {name}...")
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Store results
        model_results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'predictions': y_pred
        }

        # Print results
        print(f"{name} Results:")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")

        # Print classification report
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))

        # Plot confusion matrix
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Real Call', 'Fake Call'],
                    yticklabels=['Real Call', 'Fake Call'])
        plt.title(f'Confusion Matrix - {name}')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.savefig(f'confusion_matrix_{name.replace(" ", "_").lower()}.png')
        plt.show()

    return model_results

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, f1_score
import matplotlib.pyplot as plt

def train_ann(X_train, X_test, y_train, y_test):
    print("\n--- Training Artificial Neural Network (ANN) Model ---")

    # Scale features for neural network
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Build the model
    model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    # Compile the model
    model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['accuracy'])

    # Set up early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    )

    # Train the model
    history = model.fit(
        X_train_scaled, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=1
    )

    # Evaluate the model
    loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
    y_pred = (model.predict(X_test_scaled) > 0.5).astype(int).flatten()
    f1 = f1_score(y_test, y_pred)

    print(f"ANN Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Plot training history
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.tight_layout()
    plt.savefig('ann_training_history.png')
    plt.show()

    return model, scaler



# OPTIMIZATION: Changed from GridSearchCV to RandomizedSearchCV with fewer iterations
def tune_model(X_train, y_train, best_model_name):
    print(f"\n--- Quick Tuning for {best_model_name} Model ---")

    if best_model_name == 'Random Forest':
        model = RandomForestClassifier(random_state=42, n_jobs=-1)
        param_distributions = {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5]
        }
    elif best_model_name == 'Gradient Boosting':
        model = HistGradientBoostingClassifier(random_state=42)
        param_distributions = {
            'max_iter': [50, 100, 200],
            'learning_rate': [0.05, 0.1, 0.2],
            'max_depth': [3, 5, None]
        }

    # Create RandomizedSearchCV with fewer iterations and fewer CV folds
    random_search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_distributions,
        n_iter=10,  # Only try 10 combinations instead of all possible combinations
        cv=3,       # Use 3-fold CV instead of 5-fold
        scoring='f1',
        n_jobs=-1,  # Use all available cores
        verbose=1,
        random_state=42
    )

    # Fit RandomizedSearchCV
    print("Starting hyperparameter tuning (this will be much faster)...")
    random_search.fit(X_train, y_train)

    # Print best parameters
    print("Best Parameters:", random_search.best_params_)
    print("Best F1 Score:", random_search.best_score_)

    return random_search.best_estimator_

# Improved function to get feature importance
def get_feature_importance(model, X, y, random_state=42):
    """
    Get feature importance for any model, falling back to permutation importance
    when built-in importance is not available
    """
    feature_importance = None

    # Try standard feature_importances_ (for tree-based models like RandomForest)
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})

    # Try coef_ for linear models like LogisticRegression
    elif hasattr(model, 'coef_'):
        importances = np.abs(model.coef_[0])
        feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})

    # If HistGradientBoostingClassifier, we can try to use its built-in method if available
    elif hasattr(model, '_compute_feature_importances'):
        try:
            importances = model._compute_feature_importances()
            feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})
            print("Using built-in feature importance method")
        except:
            print("Built-in feature importance failed, falling back to permutation importance")
            # Permutation importance will be used below

    # Fallback to permutation importance for any model (including HistGradientBoosting)
    if feature_importance is None:
        print("Using permutation importance (this may take a moment)...")
        perm_importance = permutation_importance(model, X, y, n_repeats=5, random_state=random_state)
        importances = perm_importance.importances_mean
        feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})

    # Sort by importance
    feature_importance = feature_importance.sort_values('importance', ascending=False)
    return feature_importance

# Visualize feature importance
def visualize_feature_importance(feature_importance, model_name, top_n=20):
    if feature_importance is None:
        print("Feature importance not available for this model.")
        return

    # Plot the top N most important features
    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=feature_importance.head(top_n))
    plt.title(f'Top {top_n} Features - {model_name}')
    plt.tight_layout()
    plt.savefig(f'feature_importance_{model_name.replace(" ", "_").lower()}.png')
    plt.show()

# Evaluate model performance
def evaluate_final_model(model, X_test, y_test):
    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Print results
    print("\n--- Final Model Evaluation ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Real Call', 'Fake Call'],
                yticklabels=['Real Call', 'Fake Call'])
    plt.title('Confusion Matrix - Final Model')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('confusion_matrix_final_model.png')
    plt.show()

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'predictions': y_pred
    }

# Load data
file_path = '911_Calls_for_Service csv_1.csv'
df = load_data(file_path)

# Preprocess data
data = preprocess_data(df)

# Explore data
data = explore_data(data)

# Visualize data
data = visualize_data(data)

# Engineer features
X, y = engineer_features(data)

# Split data
X_train, X_test, y_train, y_test = split_data(X, y)

# Build and evaluate models
model_results = build_models(X_train, X_test, y_train, y_test)

# Find the best model based on F1 score
best_model_name = max(model_results, key=lambda k: model_results[k]['f1_score'])
print(f"\nBest model based on F1 score: {best_model_name}")

# Tune the best model
tuned_model = tune_model(X_train, y_train, best_model_name)

# Evaluate the tuned model
final_results = evaluate_final_model(tuned_model, X_test, y_test)

# Get and visualize feature importance
feature_importance = get_feature_importance(tuned_model, X_test, y_test)  # Pass y_test for permutation importance
visualize_feature_importance(feature_importance, f"Final {best_model_name}")
print("\nModel training and evaluation completed!")

def interpret_tfidf_features(data):
    # Refit a TF-IDF vectorizer with the same parameters
    tfidf = TfidfVectorizer(max_features=100, stop_words='english')
    tfidf.fit(data['description'].fillna('MISSING_DESCRIPTION'))

    # Create the reverse mapping
    vocab = tfidf.vocabulary_
    reverse_vocab = {v: k for k, v in vocab.items()}

    # Print out important features
    print("Feature desc_22 represents:", reverse_vocab.get(22, "Unknown term"))
    print("Feature desc_48 represents:", reverse_vocab.get(48, "Unknown term"))
    print("Feature desc_30 represents:", reverse_vocab.get(30, "Unknown term"))

    return reverse_vocab

# Call the function with your data
reverse_vocab = interpret_tfidf_features(data)  # Make sure 'data' is your DataFrame

